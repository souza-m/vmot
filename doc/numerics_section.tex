%% LyX 2.3.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{luainputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem{example}[thm]{\protect\examplename}

\makeatother

\usepackage{babel}
\providecommand{\examplename}{Example}
\providecommand{\theoremname}{Theorem}

\begin{document}

\paragraph{Numeric examples.}

(To be moved to the Introduction Section.) In the numerical section,
we develop several examples of robust pricing problems designed to
show the improvement allowed by our result. We consider cost functions
based on the the basket call option and a theoretical cross-product
contract. Some of the examples use uniform or normal marginal distributions,
while some use real-world marginal distributions implied by option
prices of individual assets at two distinct future points in time.

In this section, we solve several versions of the problem of robust
pricing of a financial contract involving the prices of a pair of
assets at two distinct points in time. We apply the framework of \cite{eckstein_kupper_computational_2021},
further developed in --EcksteinGuoLimObloj21-- and analyze potential
performance improvements allowed by our result about the geometry
of optimal couplings. Our tests run accross different dimensions,
cost functions and marginal shapes, including marginals derived from
actual stock market option prices, as detailed below.

The model introduced in --EcksteinKupper2021-- uses penalyzation
and neural network optimization to solve the dual problem of VMOT.
--EcksteinGuoLimObloj21-- introduces the martingale constraint to
the optimal transport problem. More precisely, the model approximates
the potential functions $\left(\phi_{i},\psi_{i},h_{i}\right)_{i}$
of equation 2.6 using neural networks and use penalyzation to impose
condition 2.7. The novelty of our method is in the improvement of
$\theta$, the sampling measure used for penalyzation: since we know
that a solution (unique in the case of strict submodularity) is attained
with monotone coupling of the $X$ marginals $\left(\vec{\mu}_{i}\right)$,
we determine $\theta^{1}$, the $X$-side of $\theta$, by those marginals.
For the sake of comparison, we run tests with and without this improvement.
For simplicity, we fix the penalyzation function as $L_{2}$ and the
penalization parameter $\gamma=1000$.
\begin{example}
(Basket option cost function, uniform marginals.) Let $d=2$ and our
cost function and marginals be 
\begin{align*}
c\left(X_{1},Y\right) & =\left(X_{1}+2X_{2}-1\right)^{+}+\left(Y_{1}+2Y_{2}-1\right)^{+}\\
X_{1},X_{2} & \sim U\left[-1,1\right]\\
Y_{1},Y_{2} & \sim U\left[-2,2\right]
\end{align*}
\end{example}
It is useful to calculate {[}bounds for $p^{+}$ and $p^{-}$? --
check conditions for monotone coupling on $Y${]} $p^{+}$ and $p^{-}$
as references for the numerical output. For the maximum, consider
$\hat{\pi}$ with positive monotone coupling on both $\hat{\pi}^{1}$
and $\hat{\pi}^{2}$. We calculate the maximum in this particular
case as 
\begin{align*}
p_{\hat{\pi}}^{+} & =p_{X,\hat{\pi}}^{+}+p_{Y,\hat{\pi}}^{+}
\end{align*}
 where, using the formula on A1 below, 
\begin{align*}
p_{X,\hat{\pi}}^{+} & =\frac{1}{3}\\
p_{Y,\hat{\pi}}^{+} & =\frac{25}{24}
\end{align*}

Thus, {[}a lower bound for $p^{+}$ is{]} $p^{+}=\frac{1}{3}+\frac{25}{24}=\frac{11}{8}=1.375$.
For the minimum, let us consider $\tilde{\pi}$ with negative monotone
coupling on both $\tilde{\pi}^{1}$ and $\tilde{\pi}^{2}$. Now we
have, again from A1,
\begin{align*}
p_{X,\tilde{\pi}}^{-} & =0\\
p_{Y,\tilde{\pi}}^{-} & =\frac{1}{8}
\end{align*}
 

Thus, {[}an upper bound for $p^{-}$ is{]} $p^{-}=\frac{1}{8}=0.125$.
Our first step in the computation process is to build two samples
from the theoretical distributions $\mu_{0}$ and $\theta$ in equation
2.3 of \cite{eckstein_kupper_computational_2021}. We repeat the target
function to be optimized here, with adapted notation. We want to minimize
or maximize
\begin{align*}
 & \int\left(\sum_{i}f_{i}+\sum_{i}g_{i}\right)d\mu_{0}+\int\beta_{\gamma}\left(c-\varphi\right)d\theta\\
 & =\sum_{i}\left(\int f_{i}d\mu_{i}+\int g_{i}d\nu_{i}\right)+\int\beta_{\gamma}\left(c-\varphi\right)d\theta
\end{align*}

Notice that we are using the fact that $\int\sum_{i}h_{i}\left(x\right).\left(y_{i}-x_{i}\right)d\mu_{0}=0$
to eliminate this term from the first integral in the equation above.
Given the separation in the first term of the RHS, $\mu_{0}$ can
be simply taken as the independent coupling of the marginals. As for
$\theta$, the only requirement besides the marginals is that a true
solution $\pi^{*}$ be absolutely convex with respect to $\theta$.
In its most general form, $\theta$ is composed of the independent
coupling of the marginals. A more elaborate version comes from using
our main result, when we set $\theta^{1}$ to be monotonically coupled.
We use both choices of $\theta$ and compare the outputs. Sample sizes
are $100$k in both cases. The graph below shows the convergence of
the numeric value for the number of iterations in the horizontal axis.
The shaded grey area covers $\pm1$ standard deviations of the numeric
outputs.

{[}graph: convergence max{]}

{[}graph: convergence min{]}

Results after $200$ iterations.

Maximization.

\begin{tabular}{|c|c|c|}
\hline 
Coupling & Independent & Positive $\theta^{1}$\tabularnewline
\hline 
\hline 
Target & \multicolumn{2}{c|}{1.375}\tabularnewline
\hline 
Dual approx.value & 1.2807 & 1.3170\tabularnewline
\hline 
Standard deviation & 0.0254 & 0.0288\tabularnewline
\hline 
Penalty & 0.0174 & 0.0055\tabularnewline
\hline 
\end{tabular}

Minimization.

\begin{tabular}{|c|c|c|}
\hline 
Coupling & Independent & Negative $\theta^{1}$\tabularnewline
\hline 
\hline 
Target & \multicolumn{2}{c|}{0.125}\tabularnewline
\hline 
Dual approx.value & 0.1433 & 0.1311\tabularnewline
\hline 
Standard deviation & 0.0230 & 0.0251\tabularnewline
\hline 
Penalty & 0.0082 & 0.0075\tabularnewline
\hline 
\end{tabular}
\begin{example}
(Covariance cost function, normal marginals.) The general form for
our cost function and marginals is
\begin{align*}
c\left(X,Y\right) & =\sum_{i}\sum_{j>i}a_{ij}X_{i}X_{j}+b_{ij}Y_{i}Y_{j}\\
X_{i} & \sim N\left(0,\sigma_{i}^{2}\right)\\
Y_{i} & \sim N\left(0,\rho_{i}^{2}\right)
\end{align*}
\end{example}
We start with $d=2$ and set
\begin{align*}
c\left(X,Y\right) & =Y_{1}Y_{2}\\
\left(\sigma_{1},\sigma_{2}\right) & =\left(2,1\right)\\
\left(\rho_{1},\rho_{2}\right) & =\left(3,4\right)
\end{align*}

Proposition 3.7 gives us the exact solution 
\[
p^{+}=2*1+\sqrt{3^{2}-2^{2}}\sqrt{4^{2}-1^{2}}\sim10.6603
\]

To avoid noisy behavior in the potential functions, we sample from
normal distributions clipped at $\pm4\sigma$. Sample sizes are $100$k.
To provide a reference for the convergence process, we also use the
fact that an optimal $\theta^{*}$ is be given by {[}reference{]}.
The value convergence using the three distributions -- with independent
and positively monotone $\theta^{1}$ and with full optimal coupling
-- is shown below.

{[}graph: convergence{]}

The convergence graph shows similar pattern between independent and
monotone coupling on $\theta^{1}$. {[}comment{]}

We construct candidate distributions $\hat{\pi}_{\text{independent}}$
and $\hat{\pi}_{\text{monotone}}$ derived from the numerically constructed
potential functions according to \cite{eckstein_kupper_computational_2021},
equation 2.6.

{[}graphs: $\hat{\pi}$ for both cases{]}

Results after $2000$ iterations.

\begin{tabular}{|c|c|c|c|}
\hline 
Cupling & Independent & Positive $\theta^{1}$ & Fully optimal\tabularnewline
\hline 
\hline 
Target & \multicolumn{3}{c|}{10.6603}\tabularnewline
\hline 
Dual approx.value & 10.3836 & 10.5351 & 10.7753\tabularnewline
\hline 
Standard deviation & 0.4413 & 0.4465 & 0.3814\tabularnewline
\hline 
Penalty & 0.4135 & 0.1684 & 0.0132\tabularnewline
\hline 
\end{tabular}

\subparagraph{Higher dimensions.}

To compare performance in higher dimensions, we also run examples
with $d=3$ and $d=5$. Our cost is simplified as 
\[
c\left(x,y\right)=\sum_{i}\sum_{j>i}b_{ij}y_{i}y_{j}
\]
 

For $d=3$, the cost, marginals and target maximum values are given
by
\begin{align*}
c\left(x,y\right) & =y_{1}y_{2}+2y_{1}y_{3}+\frac{1}{2}y_{2}y_{3}\\
\left(\mu_{1},\mu_{2},\mu_{3}\right) & =\left(1,2,3\right)\\
\left(\rho_{1},\rho_{2},\rho_{3}\right) & =\left(2,3,8\right)\\
p^{+} & \approx48.855
\end{align*}

Below is a graph of the convergence

{[}graph: convergence{]}

{[}comment{]}

Results after $2000$ iterations.

\begin{tabular}{|c|c|c|}
\hline 
Cupling & Independent & Positive $\theta^{1}$\tabularnewline
\hline 
\hline 
Target & \multicolumn{2}{c|}{48.855}\tabularnewline
\hline 
Dual approx.value &  & \tabularnewline
\hline 
Standard deviation &  & \tabularnewline
\hline 
Penalty &  & \tabularnewline
\hline 
\end{tabular}

For the $d=5$, our setting is
\begin{align*}
c\left(x,y\right) & =\sum_{i}\sum_{j>i}y_{i}y_{j}\\
\left(\mu_{1},\mu_{2},\mu_{3},\mu_{4},\mu_{5}\right) & =\left(1,2,2,3,3\right)\\
\left(\rho_{1},\rho_{2},\rho_{3},\rho_{4},\rho_{5}\right) & =\left(2,3,4,5,6\right)\\
p^{+} & \approx153.751
\end{align*}

Below is a graph of the convergence

{[}graph: convergence{]}

{[}comment{]}

Results after $2000$ iterations.

\begin{tabular}{|c|c|c|}
\hline 
Cupling & Independent & Positive $\theta^{1}$\tabularnewline
\hline 
\hline 
Target & \multicolumn{2}{c|}{153.751}\tabularnewline
\hline 
Dual approx.value &  & \tabularnewline
\hline 
Standard deviation &  & \tabularnewline
\hline 
Penalty &  & \tabularnewline
\hline 
\end{tabular}
\begin{example}
(Covariance cost function with real stock market marginals.) To be
developed. Include graphs of dual functions and discussion.
\end{example}

\paragraph{A1. Portfolio option price -- direct calculation.}

Let 
\begin{align*}
f\left(x_{1},x_{2}\right) & =\left(a_{1}x_{1}+a_{2}x_{2}-k\right)^{+}\\
X_{i}\sim\mu_{i} & \equiv U\left[-m,m\right],i=1,2
\end{align*}
with $a_{i}\geq0;m>0$. We are interested in the maximum and minimum
of the expected value of $f$ over all possible couplings of $\mu_{1}$
and $\mu_{2}$. Denote
\begin{align*}
p^{+} & =\max_{\pi\in\Pi\left(\mu_{1},\mu_{2}\right)}\mathbb{E}_{\pi}f\left(x\right)\\
p^{-} & =\min_{\pi\in\Pi\left(\mu_{1},\mu_{2}\right)}\mathbb{E}_{\pi}f\left(x\right)
\end{align*}

By {[}REF{]}, the maximum is attained at the positive-diagonal monotone
coupling of $\mu_{1},\mu_{2}$, named $\pi^{+}$. Since $x_{1}=x_{2}$
in the support of $\pi^{+}$, we have
\begin{align*}
p^{+} & =\int f\left(x\right)\mathbb{P}\left(x\right)d\pi^{+}\\
 & =\frac{1}{2m}\int_{-m}^{m}f\left(\left(t,t\right)\right)dt\\
 & =\frac{1}{2m}\int_{-m}^{\frac{k}{a_{1}+a_{2}}}f\left(\left(t,t\right)\right)dt+\frac{1}{2m}\int_{\frac{k}{a_{1}+a_{2}}}^{m}f\left(\left(t,t\right)\right)dt
\end{align*}

Notice that the first integral is zero, and so is the second one if
$m\leq\frac{k}{a_{1}+a_{2}}$. If $m\geq\frac{k}{a_{1}+a_{2}}$ then
we have
\begin{align*}
p^{+} & =\frac{1}{2m}\int_{\frac{k}{a_{1}+a_{2}}}^{m}f\left(\left(t,t\right)\right)dt\\
 & =\frac{1}{2m}\int_{\frac{k}{a_{1}+a_{2}}}^{m}\left[\left(a_{1}+a_{2}\right)t-k\right]dt\\
 & =\frac{a_{1}+a_{2}}{4m}\left[m^{2}-\left(\frac{k}{a_{1}+a_{2}}\right)^{2}\right]-k\left(m-\frac{k}{a_{1}+a_{2}}\right)\\
 & =\frac{a_{1}+a_{2}}{4m}\left[m^{2}-2m\frac{k}{a_{1}+a_{2}}+\left(\frac{k}{a_{1}+a_{2}}\right)^{2}\right]\\
 & =\frac{a_{1}+a_{2}}{4m}\left(m-\frac{k}{a_{1}+a_{2}}\right)^{2}
\end{align*}

Similarly, for the minimum we consider the negative-diagonal monotone
coupling $\pi^{-}$ in whose support $x_{2}=-x_{1}$. Thus we have
\begin{align*}
p^{-} & =\int f\left(x\right)\mathbb{P}\left(x\right)d\pi^{-}\\
 & =\frac{1}{2m}\int_{-m}^{m}f\left(\left(t,-t\right)\right)dt\\
 & =\frac{1}{2m}\int_{-m}^{\frac{k}{a_{1}-a_{2}}}f\left(\left(t,-t\right)\right)dt+\frac{1}{2m}\int_{\frac{k}{a_{1}-a_{2}}}^{m}f\left(\left(t,-t\right)\right)dt
\end{align*}

Now the second integral is zero and so is the first if $-m\geq\frac{k}{a_{1}-a_{2}}$,
or $m\leq\frac{k}{a_{2}-a_{1}}$. If $m\geq\frac{k}{a_{2}-a_{1}}$
then
\begin{align*}
p^{-} & =\frac{1}{2m}\int_{-m}^{\frac{k}{a_{1}-a_{2}}}f\left(\left(t,-t\right)\right)dt\\
 & =\frac{1}{2m}\int_{-m}^{\frac{k}{a_{1}-a_{2}}}\left[\left(a_{1}-a_{2}\right)t-k\right]dt\\
 & =\frac{a_{1}-a_{2}}{4m}\left[\left(\frac{k}{a_{1}-a_{2}}\right)^{2}-m^{2}\right]-k\left(\frac{k}{a_{1}-a_{2}}+m\right)\\
 & =\frac{a_{1}-a_{2}}{4m}\left[-\left(\frac{k}{a_{1}-a_{2}}\right)^{2}-m^{2}-2km\right]\\
 & =\frac{a_{2}-a_{1}}{4m}\left(m-\frac{k}{a_{2}-a_{1}}\right)^{2}
\end{align*}

\bibliographystyle{plain}
\bibliography{../../Research/REFs}

\end{document}
