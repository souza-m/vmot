%% LyX 2.3.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{luainputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem{example}[thm]{\protect\examplename}

\makeatother

\usepackage{babel}
\providecommand{\examplename}{Example}
\providecommand{\theoremname}{Theorem}

\begin{document}

\paragraph{Numeric examples.}

(To be moved to the Introduction Section.) In the Numerics section,
we develop several examples of robust pricing problems designed to
show the improvement allowed by our result. We consider cost functions
based on the the european call option applied to a basket of assets
and a theoretical covariance (or cross-product) contract. The first
two examples use hypothetical uniform and normal marginal distributions,
while the third uses real-world marginal distributions implied by
option prices of individual assets at two distinct future points in
time.

In this section, we solve several versions of the problem of robust
pricing of a financial contract involving the prices of a group of
assets at two distinct points in time. We build upon the framework
of EK21\cite{eckstein_kupper_computational_2021} and analyze potential
performance improvements allowed by our result on the geometry of
the optimal couplings. Our tests run across various dimensions, cost
functions and marginal shapes, including marginals derived from actual
stock market option prices.

{[}NOTE: discussion about $\theta$ moved to here and developed further{]}

EK21 uses neural network optimization with penalization to solve the
dual problem of Optimal Transport. More specific to our case, their
model approximate the potential functions of our equation 2.6 with
neural networks and impose condition 2.7 though penalization. The
optimization objective is summarized in a sum of integrals (equation
2.3 of EK21), which we repeat here with adapted notation. We want
to (min)maximize
\begin{align*}
\text{Loss} & =\int\varphi d\mu_{0}+\int b_{\gamma}\left(c-\varphi\right)d\theta\\
 & =\int\left(\sum_{i}\phi_{i}+\sum_{i}\psi_{i}\right)d\mu_{0}+\int b_{\gamma}\left(c-\varphi\right)d\theta\\
 & =\sum_{i}\int\phi_{i}d\mu_{i}+\sum_{i}\int\psi_{i}d\nu_{i}+\int b_{\gamma}\left(c-\varphi\right)d\theta
\end{align*}
 where $\varphi\left(x,y\right)=\sum_{i}\phi_{i}\left(x_{i}\right)+\sum_{i}\psi_{i}\left(y_{i}\right)+\sum_{i}h_{i}\left(x\right).\left(y_{i}-x_{i}\right)$.
Notice that the fact that $\int\sum_{i}h_{i}\left(x\right).\left(y_{i}-x_{i}\right)d\mu_{0}=0$,
from the martingale condition, allows us to eliminate this term from
the first integral. Given the separation in the first term of the
RHS, $\mu_{0}$ can be simply taken as the independent coupling of
the marginals. Computationally, each of $\phi_{i},\psi_{i},h_{i}$
is replaced by some approximation $\phi_{i}^{m},\psi_{i}^{m},h_{i}^{m}$
implemented as a neural network with an internal size measured by
$m$. The calibration of the neural networks is performed through
gradient descent methods referenced on a loss function. The last integral
adds a penalization to the loss function, imposing the appropriate
pointwise inequality between $\varphi$ and $c$.

Our main point of interest is in the sampling measure $\theta$ used
for penalization. While the only requirement on $\theta$ besides
is that a true solution $\pi^{*}$ be absolutely convex with respect
to it, a better accuracy is expected in general when $\theta$ is
closer to a true solution -- see brief discussion in sections 4.4
and 4.5 of EK21. In its most general form, $\theta$ is composed of
the independent coupling of the marginals. The novelty of our method
is in the improvement of $\theta$: since we know that a solution
(unique in the case of strict submodularity) is attained with monotone
coupling of the $X$ marginals $\left(\vec{\mu}_{i}\right)$, we determine
$\theta^{1}$, the $X$-side of $\theta$, by those marginals. In
the special case of normal marginals, we improve $\theta$ a little
further by applying additional knowledge about the full coupling of
the marginals, including the $Y$ side. It can be possible to improve
$\theta$ even further and in more general cases. For instance, we
could achieve great improvements with the knowledge of the conditional
distribution of $Y$ given $X$, that is, $\pi_{x}$ for a fixed $x$.
This topic remains for future research. The purpose here is to run
tests with and without the known improvements that we have at hand.
We fix the penalization function $b$ (see discussion in the reference)
as
\begin{align*}
b\left(t\right) & =\frac{1}{\gamma}\frac{1}{2}\left(\left(\gamma t\right)^{+}\right)^{2} & \gamma=1000
\end{align*}

\begin{example}
(Basket option cost function, uniform marginals.) Our first cost function
mimics the payoff of a contract similar to an European call option
applied to a basket composed of assets. That is, a cost function with
general form 
\begin{align*}
c\left(X,Y\right) & =\left(\sum_{i=1}^{d}a_{i}X_{i}-k_{1}\right)^{+}+\left(\sum_{i=1}^{d}b_{i}Y_{i}-k_{2}\right)^{+}
\end{align*}

Our marginals are uniform distributions centered at zero. Set $d=2$
and 
\begin{align*}
c\left(X_{1},Y\right) & =\left(X_{1}+2X_{2}-1\right)^{+}+\left(Y_{1}+2Y_{2}-1\right)^{+}\\
X_{1},X_{2} & \sim U\left[-1,1\right]\\
Y_{1},Y_{2} & \sim U\left[-2,2\right]
\end{align*}
\end{example}
We are interested in both upper and lower bounds. It is useful to
calculate $p^{+}$ and $p^{-}$ as references for the numerical output
{[}check conditions for monotone coupling on $Y${]} . For the maximum
(minimum) we take $\hat{\pi}$ with positive (negative) monotone coupling
on both $\hat{\pi}^{1}$ and $\hat{\pi}^{2}$. We calculate the bounds
as 
\begin{align*}
p_{\hat{\pi}}^{+} & =p_{X,\hat{\pi}}^{+}+p_{Y,\hat{\pi}}^{+}\\
p_{\hat{\pi}}^{-} & =p_{X,\hat{\pi}}^{-}+p_{Y,\hat{\pi}}^{-}
\end{align*}
 where, using the formula on A1 below, 
\begin{align*}
p_{X,\hat{\pi}}^{+} & =\frac{1}{3}; & p_{Y,\hat{\pi}}^{+} & =\frac{25}{24}\\
p_{Y,\hat{\pi}}^{-} & =0; & p_{Y,\tilde{\pi}}^{-} & =\frac{1}{8}
\end{align*}

Thus, our theoretical reference bounds for the numerical approximation
are $p^{+}=\frac{1}{3}+\frac{25}{24}=\frac{11}{8}=1.375$ and $p^{-}=\frac{1}{8}=0.125$.
Our first step in the computation process is to build two samples
from the theoretical distributions $\mu_{0}$ and $\theta$ in equation
2.3 of --EcksteinKupper21--. {[}NOTE: refer to the discussion about
$\theta$ in the section introduction{]} A more elaborate version
comes from using our main result, when we set $\theta^{1}$ to be
determined by the monotonic coupling of $\left(\vec{\mu}_{i}\right)$
(positive or negative according to the objective). We use both choices
of $\theta$ and compare the outputs. Sample sizes are $100$k in
both cases. The graph below shows the convergence of the numeric value
for the number of iterations in the horizontal axis. The shaded grey
area covers $\pm1$ standard deviations of the numeric outputs.

{[}graph: convergence max{]}

{[}graph: convergence min{]}

The following table summarizes the results after $200$ iterations.

Maximization.

\begin{tabular}{|c|c|c|}
\hline 
Coupling & Independent & Positive $\theta^{1}$\tabularnewline
\hline 
\hline 
Target & \multicolumn{2}{c|}{1.375}\tabularnewline
\hline 
Dual approx.value & 1.2807 & 1.3170\tabularnewline
\hline 
Standard deviation & 0.0254 & 0.0288\tabularnewline
\hline 
Penalty & 0.0174 & 0.0055\tabularnewline
\hline 
\end{tabular}

Minimization.

\begin{tabular}{|c|c|c|}
\hline 
Coupling & Independent & Negative $\theta^{1}$\tabularnewline
\hline 
\hline 
Target & \multicolumn{2}{c|}{0.125}\tabularnewline
\hline 
Dual approx.value & 0.1433 & 0.1311\tabularnewline
\hline 
Standard deviation & 0.0230 & 0.0251\tabularnewline
\hline 
Penalty & 0.0082 & 0.0075\tabularnewline
\hline 
\end{tabular}\\

\begin{example}
(Covariance cost function, normal marginals.) For our second example,
we consider a cost function inspired by a theoretical contract with
payoff proportional to the covariance of two assets. The general form
is
\begin{align*}
c\left(X,Y\right) & =\sum_{i}\sum_{j>i}a_{ij}X_{i}X_{j}+b_{ij}Y_{i}Y_{j}
\end{align*}
\end{example}
Now our marginals follow normal distributions with parameters 
\begin{align*}
X_{i} & \sim N\left(0,\sigma_{i}^{2}\right)\\
Y_{i} & \sim N\left(0,\rho_{i}^{2}\right)
\end{align*}

We start with $d=2$ and set
\begin{align*}
c\left(X,Y\right) & =Y_{1}Y_{2}\\
\left(\sigma_{1},\sigma_{2}\right) & =\left(2,1\right)\\
\left(\rho_{1},\rho_{2}\right) & =\left(3,4\right)
\end{align*}

By symmetry, we only need to look at the maximization problem. Proposition
3.7 gives us the exact solution to be used as a reference for the
maximum price 
\[
p^{+}=2*1+\sqrt{3^{2}-2^{2}}\sqrt{4^{2}-1^{2}}\sim10.6603
\]

To avoid noisy behavior in the potential functions, we sample from
normal distributions clipped at $\pm4\sigma$. As before, we run our
optimization with the most general sampling measure $\theta$ given
by the independent coupling of the marginals and with an improved
measure that uses our result, namely, with $\theta^{1}$ determined
by the monotone coupling of $\left(\vec{\mu}_{i}\right)$. A futher
improvement is allowed by the fact that we are using normal marginals,
see (REF), which gives us a third, potentially even more efficient
$\theta$ where $\theta^{1}$ is given as before and $\theta^{2}$
follows a joint normal with parameters given by the reference (develop).
The value convergence using the three sampling measures is shown below.
Sample sizes are $100$k.

{[}graph: convergence{]}

The convergence graph shows similar pattern between independent and
monotone coupling on $\theta^{1}$. (comment)

To provide a visual illustration of the coupling process, we construct
examples of distributions from the potential functions generated in
the numerical procedure, according to equation 2.6 of --EcksteinKupper2021--
\cite{eckstein_kupper_computational_2021}.

{[}graphs: $\hat{\pi}$ for both cases{]}

Results after $2000$ iterations.

\begin{tabular}{|c|c|c|c|}
\hline 
Cupling & Independent & Positive $\theta^{1}$ & Positive $\theta^{1}$ and optimal $\theta^{2}$\tabularnewline
\hline 
\hline 
Target & \multicolumn{3}{c|}{10.6603}\tabularnewline
\hline 
Dual approx.value & 10.3836 & 10.5351 & 10.7753\tabularnewline
\hline 
Standard deviation & 0.4413 & 0.4465 & 0.3814\tabularnewline
\hline 
Penalty & 0.4135 & 0.1684 & 0.0132\tabularnewline
\hline 
\end{tabular}

\subparagraph{Higher dimensions.}

To compare performance in higher dimensions, we also run examples
with $d=3$ and $d=5$ (to do: change second to $d=10$). Our cost
is simplified as 
\[
c\left(x,y\right)=\sum_{i}\sum_{j>i}b_{ij}y_{i}y_{j}
\]
 

For $d=3$, the cost, marginals and target maximum values are given
by
\begin{align*}
c\left(x,y\right) & =y_{1}y_{2}+2y_{1}y_{3}+\frac{1}{2}y_{2}y_{3}\\
\left(\mu_{1},\mu_{2},\mu_{3}\right) & =\left(1,2,3\right)\\
\left(\rho_{1},\rho_{2},\rho_{3}\right) & =\left(2,3,8\right)\\
p^{+} & \approx48.855
\end{align*}

Below is a graph of the convergence

{[}graph: convergence{]}

{[}comment{]}

Results after $2000$ iterations. (update)

\begin{tabular}{|c|c|c|}
\hline 
Cupling & Independent & Positive $\theta^{1}$\tabularnewline
\hline 
\hline 
Target & \multicolumn{2}{c|}{48.855}\tabularnewline
\hline 
Dual approx.value &  & \tabularnewline
\hline 
Standard deviation &  & \tabularnewline
\hline 
Penalty &  & \tabularnewline
\hline 
\end{tabular}

For the $d=5$, our setting is
\begin{align*}
c\left(x,y\right) & =\sum_{i}\sum_{j>i}y_{i}y_{j}\\
\left(\mu_{1},\mu_{2},\mu_{3},\mu_{4},\mu_{5}\right) & =\left(1,2,2,3,3\right)\\
\left(\rho_{1},\rho_{2},\rho_{3},\rho_{4},\rho_{5}\right) & =\left(2,3,4,5,6\right)\\
p^{+} & \approx153.751
\end{align*}

Below is a graph of the convergence

{[}graph: convergence{]}

{[}comment{]}

Results after $2000$ iterations. (update)

\begin{tabular}{|c|c|c|}
\hline 
Cupling & Independent & Positive $\theta^{1}$\tabularnewline
\hline 
\hline 
Target & \multicolumn{2}{c|}{153.751}\tabularnewline
\hline 
Dual approx.value &  & \tabularnewline
\hline 
Standard deviation &  & \tabularnewline
\hline 
Penalty &  & \tabularnewline
\hline 
\end{tabular}\\

\begin{example}
(Covariance cost function with real stock market marginals.) To be
developed. Include graphs of dual functions and discussion.
\end{example}

\paragraph{A1. Portfolio option price -- direct calculation.}

Let 
\begin{align*}
f\left(x_{1},x_{2}\right) & =\left(a_{1}x_{1}+a_{2}x_{2}-k\right)^{+}\\
X_{i}\sim\mu_{i} & \equiv U\left[-m,m\right],i=1,2
\end{align*}
with $a_{i}\geq0;m>0$. We are interested in the maximum and minimum
of the expected value of $f$ over all possible couplings of $\mu_{1}$
and $\mu_{2}$. Denote
\begin{align*}
p^{+} & =\max_{\pi\in\Pi\left(\mu_{1},\mu_{2}\right)}\mathbb{E}_{\pi}f\left(x\right)\\
p^{-} & =\min_{\pi\in\Pi\left(\mu_{1},\mu_{2}\right)}\mathbb{E}_{\pi}f\left(x\right)
\end{align*}

By {[}REF{]}, the maximum is attained at the positive-diagonal monotone
coupling of $\mu_{1},\mu_{2}$, named $\pi^{+}$. Since $x_{1}=x_{2}$
in the support of $\pi^{+}$, we have
\begin{align*}
p^{+} & =\int f\left(x\right)\mathbb{P}\left(x\right)d\pi^{+}\\
 & =\frac{1}{2m}\int_{-m}^{m}f\left(\left(t,t\right)\right)dt\\
 & =\frac{1}{2m}\int_{-m}^{\frac{k}{a_{1}+a_{2}}}f\left(\left(t,t\right)\right)dt+\frac{1}{2m}\int_{\frac{k}{a_{1}+a_{2}}}^{m}f\left(\left(t,t\right)\right)dt
\end{align*}

Notice that the first integral is zero, and so is the second one if
$m\leq\frac{k}{a_{1}+a_{2}}$. If $m\geq\frac{k}{a_{1}+a_{2}}$ then
we have
\begin{align*}
p^{+} & =\frac{1}{2m}\int_{\frac{k}{a_{1}+a_{2}}}^{m}f\left(\left(t,t\right)\right)dt\\
 & =\frac{1}{2m}\int_{\frac{k}{a_{1}+a_{2}}}^{m}\left[\left(a_{1}+a_{2}\right)t-k\right]dt\\
 & =\frac{a_{1}+a_{2}}{4m}\left[m^{2}-\left(\frac{k}{a_{1}+a_{2}}\right)^{2}\right]-k\left(m-\frac{k}{a_{1}+a_{2}}\right)\\
 & =\frac{a_{1}+a_{2}}{4m}\left[m^{2}-2m\frac{k}{a_{1}+a_{2}}+\left(\frac{k}{a_{1}+a_{2}}\right)^{2}\right]\\
 & =\frac{a_{1}+a_{2}}{4m}\left(m-\frac{k}{a_{1}+a_{2}}\right)^{2}
\end{align*}

Similarly, for the minimum we consider the negative-diagonal monotone
coupling $\pi^{-}$ in whose support $x_{2}=-x_{1}$. Thus we have
\begin{align*}
p^{-} & =\int f\left(x\right)\mathbb{P}\left(x\right)d\pi^{-}\\
 & =\frac{1}{2m}\int_{-m}^{m}f\left(\left(t,-t\right)\right)dt\\
 & =\frac{1}{2m}\int_{-m}^{\frac{k}{a_{1}-a_{2}}}f\left(\left(t,-t\right)\right)dt+\frac{1}{2m}\int_{\frac{k}{a_{1}-a_{2}}}^{m}f\left(\left(t,-t\right)\right)dt
\end{align*}

Now the second integral is zero and so is the first if $-m\geq\frac{k}{a_{1}-a_{2}}$,
or $m\leq\frac{k}{a_{2}-a_{1}}$. If $m\geq\frac{k}{a_{2}-a_{1}}$
then
\begin{align*}
p^{-} & =\frac{1}{2m}\int_{-m}^{\frac{k}{a_{1}-a_{2}}}f\left(\left(t,-t\right)\right)dt\\
 & =\frac{1}{2m}\int_{-m}^{\frac{k}{a_{1}-a_{2}}}\left[\left(a_{1}-a_{2}\right)t-k\right]dt\\
 & =\frac{a_{1}-a_{2}}{4m}\left[\left(\frac{k}{a_{1}-a_{2}}\right)^{2}-m^{2}\right]-k\left(\frac{k}{a_{1}-a_{2}}+m\right)\\
 & =\frac{a_{1}-a_{2}}{4m}\left[-\left(\frac{k}{a_{1}-a_{2}}\right)^{2}-m^{2}-2km\right]\\
 & =\frac{a_{2}-a_{1}}{4m}\left(m-\frac{k}{a_{2}-a_{1}}\right)^{2}
\end{align*}

\bibliographystyle{plain}
\bibliography{REFs}

\end{document}
