#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\begin_modules
theorems-std
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Paragraph

\size normal
Discussion: sampling vs.
 quantile approach.
\end_layout

\begin_layout Standard
We describe the numerical penalization method being currently used, with
 emphasis on the sampling of points for penalization and discuss an alternative
 option to consider.
 To approximate a solution of equation 2.6 with constraint 2.7, we want to
 minimize 
\begin_inset Formula 
\[
\sum_{i}\int\phi_{i}\mu_{i}+\sum_{i}\int\psi_{i}d\nu_{i}+\int b_{\gamma}\left(c-\varphi\right)d\theta
\]

\end_inset

 where 
\begin_inset Formula $b_{\gamma}$
\end_inset

 is a penalization function defined as
\begin_inset Formula 
\[
b_{\gamma}\left(t\right)=\frac{1}{\gamma}\frac{1}{2}\left[\left(\gamma t\right)^{+}\right]^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Computationally, each function 
\begin_inset Formula $\phi_{i},\psi_{i},h_{i}$
\end_inset

 is implemented as a neural network that takes any floating-point number
 (
\begin_inset Formula $\phi_{i},\psi_{i}$
\end_inset

) or vector (
\begin_inset Formula $h_{i}$
\end_inset

) as input.
 The sum of integrals above becomes a discrete sum (to be detailed later),
 and is referenced to a gradient descent algorithm as a loss function (that
 is, the one to be minimized numerically).
 The proposed alternative is to work on the quantile domain.
 Let 
\begin_inset Formula $F_{i},G_{i}$
\end_inset

 be the cumulative distribution functions of 
\begin_inset Formula $x_{i},y_{i}$
\end_inset

 (assumed to be known) and define
\begin_inset Formula 
\begin{alignat*}{1}
\hat{x}_{i} & =F\left(x_{i}\right)\\
\hat{y}_{i} & =G\left(y_{i}\right)\\
\hat{\phi}_{i}\left(\hat{y_{i}}\right) & =\phi_{i}\left(y_{i}\right)\\
\hat{\psi_{i}}\left(\hat{y_{i}}\right) & =\psi_{i}\left(y_{i}\right)\\
\hat{h}_{i}\left(\hat{x}\right) & =h_{i}\left(x\right)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
Now each 
\begin_inset Formula $\hat{\phi}_{i},\hat{\psi}_{i},\hat{h}_{i}$
\end_inset

 is implemented by a neural network instead of their primitives.
 Instead of random sampling, we use a grid of quantiles.
 Our marginals are uniform discrete probabilities depending on the choice
 of grid size.
 The sampling measure 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is a discrete coupling of uniform discrete marginals.
 Some advantages of this 
\begin_inset Quotes eld
\end_inset

quantile
\begin_inset Quotes erd
\end_inset

 approach are
\end_layout

\begin_layout Itemize
It spreads the area covered by the training evenly.
 This is critical in the high dimension case where much of the area can
 be left uncovered with random sampling.
\end_layout

\begin_layout Itemize
The neural network will potentially perform better in the interval 
\begin_inset Formula $\left[0,1\right]$
\end_inset

 than in the line.
 A good reason to believe that is the tail noise observed in the normal-marginal
s example.
 To be checked.
\end_layout

\begin_layout Itemize
Potential to find ways to update of the sampling measure 
\begin_inset Formula $\hat{\theta}$
\end_inset

.
\end_layout

\begin_layout Subparagraph*
Approximations to the penalization integral: description and comparison.
\end_layout

\begin_layout Standard
We approximate the penalty term 
\begin_inset Formula $\varepsilon=\int b_{\gamma}\left(c-\varphi\right)d\theta$
\end_inset

 with a sum, as described below.
\end_layout

\begin_layout Enumerate
Sampling.
 Let 
\begin_inset Formula $\mathcal{X}_{i}=\left\{ x_{i}^{1},\ldots x_{i}^{n}\right\} ,\mathcal{Y}_{i}=\left\{ y_{i}^{1},\ldots y_{i}^{n}\right\} $
\end_inset

 be (simulated) samples, 
\begin_inset Formula $i=1,\ldots,d$
\end_inset

.
 Note 1: the marginal probabilities are expressed in the samples; their
 construction is the only place where the marginal probabilities are ever
 considered.
 Note 2: In the empirical example, even though we are using actual market
 data, the method first builds marginal distributions of random, future
 prices and then generates a sample from those distributions.
 Hence, the final sample is simulated in any case.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Current method.
\end_layout

\begin_deeper
\begin_layout Standard
First step is to build a common sample 
\begin_inset Formula $\Theta_{1}$
\end_inset

 of 
\begin_inset Formula $2d$
\end_inset

-dimension points.
 In the independent case, we wrap the points arbitrarily as (for instance)
 
\begin_inset Formula 
\[
\Theta_{1}=\left\{ \left(x_{1}^{1},\ldots,x_{d}^{1},y_{1}^{1},\ldots,y_{d}^{1}\right),\ldots,\left(x_{1}^{n},\ldots,x_{d}^{n},y_{1}^{n},\ldots,y_{d}^{n}\right)\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
In the monotone coupling case, the 
\begin_inset Formula $x_{i}^{k}$
\end_inset

's are first ordered as 
\begin_inset Formula $x_{i}^{\left(1\right)},\ldots,x_{i}^{\left(n\right)}$
\end_inset

 and then coupled in order, while the 
\begin_inset Formula $y_{i}^{k}$
\end_inset

's are coupled arbitrarily, like 
\begin_inset Formula 
\[
\Theta_{1}=\left\{ \left(x_{1}^{\left(1\right)},\ldots,x_{d}^{\left(1\right)},y_{1}^{1},\ldots,y_{d}^{1}\right),\ldots,\left(x_{1}^{\left(n\right)},\ldots,x_{d}^{\left(n\right)},y_{1}^{n},\ldots,y_{d}^{n}\right)\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
The penalty is given by
\begin_inset Formula 
\[
\varepsilon_{1}=\frac{1}{\left|\Theta_{1}\right|}\sum_{\left(x,y\right)\in\Theta_{1}}b_{\gamma}\left(c\left(x,y\right)-\varphi\left(x,y\right)\right)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Separated sampling.
 The common sample 
\begin_inset Formula $\Theta_{2}$
\end_inset

 is the product of the marginal samples.
 In the independent coupling case, the penalty is calculated as
\begin_inset Formula 
\[
\varepsilon_{2}=\frac{1}{\prod_{i}\prod_{j}\left|\mathcal{X}_{i}\right|\left|Y_{i}\right|}\sum_{x_{1}\in\mathcal{X}_{1}}\ldots\sum_{x_{d}\in\mathcal{X}_{d}}\sum_{y_{1}\in\mathcal{Y}_{1}}\ldots\sum_{y_{d}\in\mathcal{Y}_{d}}b_{\gamma}\left(c\left(x,y\right)-\varphi\left(x,y\right)\right)
\]

\end_inset

In the monotone case, the choice of 
\begin_inset Formula $x$
\end_inset

 can be seen as a function of 
\begin_inset Formula $x_{1}$
\end_inset

, therefore we can write 
\begin_inset Formula 
\[
\varepsilon_{2}=\frac{1}{\left|\mathcal{X}_{1}\right|\prod_{j}\left|Y_{i}\right|}\sum_{x_{1}\in\mathcal{X}_{1}}\sum_{y_{1}\in\mathcal{Y}_{1}}\ldots\sum_{y_{d}\in\mathcal{Y}_{d}}b_{\gamma}\left(c\left(x\left(x_{1}\right),y\right)-\varphi\left(x\left(x_{1}\right),y\right)\right)
\]

\end_inset

To keep the complexity comparable, we must have either 
\begin_inset Formula $\prod_{i}\prod_{j}\left|\mathcal{X}_{i}\right|\left|Y_{i}\right|=n^{2d}$
\end_inset

 (in the independent case) or 
\begin_inset Formula $\left|\mathcal{X}_{1}\right|\prod_{j}\left|Y_{i}\right|=n^{d+1}$
\end_inset

 (in the monotone coupling case) in the same scale as 
\begin_inset Formula $\left|\Theta_{1}\right|$
\end_inset

 in the previous method.
 Thus, suppose we used 
\begin_inset Formula $\left|\Theta_{1}\right|=1M$
\end_inset

 there.
 Here, in the independent case, we should use a comparable 
\begin_inset Formula $n=\left(1M\right)^{\nicefrac{1}{4}}\sim32$
\end_inset

 for 
\begin_inset Formula $d=2$
\end_inset

 and 
\begin_inset Formula $n=\left(1M\right)^{\nicefrac{1}{6}}=10$
\end_inset

 for 
\begin_inset Formula $d=3$
\end_inset

.
 In the monotone coupling case, we should use 
\begin_inset Formula $n=\left(1M\right)^{\nicefrac{1}{3}}=100$
\end_inset

 for 
\begin_inset Formula $d=2$
\end_inset

 and 
\begin_inset Formula $n=\left(1M\right)^{\nicefrac{1}{4}}\sim32$
\end_inset

 for 
\begin_inset Formula $d=3$
\end_inset

.
\end_layout

\begin_layout Standard
Notice that, provided the sample sets have the same size, there is no difference
 between the two methods just discussed in terms of computational effort
 (other than possibly memory allocation).
 But in the separated sampling case, given that marginal sizes are small
 specially in higher dimension, we may have some areas of the marginal distribut
ions being scarcely covered throughout 
\begin_inset Formula $\Theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Quantile method.
 The points 
\begin_inset Formula $\left(\hat{x},\hat{y}\right)$
\end_inset

 are taken from a grid in the 1-hypercube.
 So, for example, if the grid granularity is 
\begin_inset Formula $10$
\end_inset

, we divide each marginal of the cube into 10 intervals, and pick the middle
 points 
\begin_inset Formula 
\begin{align*}
\hat{x}_{i}\in\hat{\mathcal{X}}_{i} & =\left\{ 0.05,\ldots,0.95\right\} \\
\hat{y}_{i}\in\hat{\mathcal{Y}}_{i} & =\left\{ 0.05,\ldots,0.95\right\} 
\end{align*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The points 
\begin_inset Formula $\left(\hat{x},\hat{y}\right)$
\end_inset

 are attributed the discrete probability 
\begin_inset Formula $\hat{\theta}$
\end_inset

 with uniform marginals.
 The penalty is given by
\begin_inset Formula 
\[
\varepsilon_{3}=\frac{1}{\prod_{i}\prod_{j}\left|\mathcal{\hat{X}}_{i}\right|\left|\hat{Y}_{i}\right|}\sum_{\hat{x}_{1}\in\hat{\mathcal{X}}_{1}}\ldots\sum_{\hat{x}_{d}\in\hat{\mathcal{X}}_{d}}\sum_{\hat{y}_{1}\in\hat{\mathcal{Y}}_{1}}\ldots\sum_{\hat{y}_{d}\in\hat{\mathcal{Y}}_{d}}\hat{\theta}\left(\hat{x},\hat{y}\right)b_{\gamma}\left(c\left(x,y\right)-\varphi\left(x,y\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Notice that the concept of sampling measure has entered the formula through
 the probability 
\begin_inset Formula $\hat{\theta}$
\end_inset

, while previously it only influenced the construction of the sample 
\begin_inset Formula $\Theta$
\end_inset

.
 In the simplest, independent case, 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is the independent coupling, which is the uniform probability 
\begin_inset Formula $\hat{\theta}\left(\hat{x},\hat{y}\right)=\frac{1}{n^{2d}}$
\end_inset

.
 In the positive monotone coupling case, only points satisfying 
\begin_inset Formula $\hat{x}_{i}=\hat{x}_{j}\forall i,j$
\end_inset

 have positive probability.
 So, if the 
\begin_inset Formula $y$
\end_inset

-side of 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is independent, we set 
\begin_inset Formula 
\[
\hat{\theta}\left(\hat{x},\hat{y}\right)=\begin{cases}
\frac{1}{n^{d+1}} & \hat{x}_{i}=\hat{x}_{j}\forall i,j\\
0 & \text{otherwise}
\end{cases}
\]

\end_inset

There could be interesting cases where additional information is available,
 giving rise to a more elaborated and efficient 
\begin_inset Formula $\hat{\theta}$
\end_inset

.
 One topic to investigate is whether it is possible to update 
\begin_inset Formula $\hat{\theta}$
\end_inset

 with information from 
\begin_inset Formula $\hat{\varphi}$
\end_inset

 in the same fashion as in formula 2.6 of Eckstein and Kupper (2021).
\end_layout

\begin_layout Standard
Note: There might be some literature about similar quantile-based methods.
 In their brief introduction to Example 4.6, Eckstein and Kupper (2021) mention
 the following papers, not yet analyzed.
\end_layout

\begin_layout Itemize
Embrechts, P., Puccetti, G., Rüschendorf, L.
 "Model uncertainty and VaR aggregation." (2013, J.
 Bank.
 Financ.)
\end_layout

\begin_layout Itemize
Puccetti, G., Rüschendorf, L.
 "Computation of sharp bounds on the distribution of a function of dependent
 risks." (J.
 Comput.
 Appl.
 Math., 2012)
\end_layout

\end_deeper
\end_body
\end_document
