#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\begin_modules
theorems-std
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Paragraph

\size normal
Discussion: sampling vs.
 quantile approach.
\end_layout

\begin_layout Standard
We describe the numerical penalization method being currently used, with
 emphasis on the sampling of points for penalization, and discuss an alternative
 option to consider.
 To approximate a solution of equation 2.6 with constraint 2.7, we want to
 minimize 
\begin_inset Formula 
\[
\sum_{i}\int\phi_{i}\mu_{i}+\sum_{i}\int\psi_{i}d\nu_{i}+\int b_{\gamma}\left(c-\varphi\right)d\theta
\]

\end_inset

 where 
\begin_inset Formula $b_{\gamma}$
\end_inset

 is a penalization function defined as
\begin_inset Formula 
\[
b_{\gamma}\left(t\right)=\frac{1}{\gamma}\frac{1}{2}\left[\left(\gamma t\right)^{+}\right]^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Compotationally, each function 
\begin_inset Formula $\phi_{i},\psi_{i},h_{i}$
\end_inset

 is implemented as a neural network that takes any floating-point number
 (
\begin_inset Formula $\phi_{i},\psi_{i}$
\end_inset

) or vector (
\begin_inset Formula $h_{i}$
\end_inset

) as input.
 The sum of integrals above becomes a discrete sum (to be detailed later),
 and is referenced to a gradient descent algorithm as a loss function (that
 is, the one to be minimized numerically).
 The proposed alternative is to work on the quantile domain.
 Let 
\begin_inset Formula $F_{i},G_{i}$
\end_inset

 be the cumulative distribution functions of 
\begin_inset Formula $x_{i},y_{i}$
\end_inset

 (assumed to be known) and define
\begin_inset Formula 
\begin{alignat*}{1}
\hat{x}_{i} & =F\left(x_{i}\right)\\
\hat{y}_{i} & =G\left(y_{i}\right)\\
\hat{\phi}_{i}\left(\hat{y_{i}}\right) & =\phi_{i}\left(y_{i}\right)\\
\hat{\psi_{i}}\left(\hat{y_{i}}\right) & =\psi_{i}\left(y_{i}\right)\\
\hat{h}_{i}\left(\hat{x}\right) & =h_{i}\left(x\right)
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
Now each 
\begin_inset Formula $\hat{\phi}_{i},\hat{\psi}_{i},\hat{h}_{i}$
\end_inset

 is implemented by a neural network instead of their primitives.
 Alternatively to random sampling, we run our model from a grid of quantiles.
 Our marginals are uniform discrete probabilities depending on the choice
 of grid size.
 The sampling measure 
\begin_inset Formula $\hat{\theta}$
\end_inset

 is a discrete coupling of uniform discrete marginals.
 Advantages of this 
\begin_inset Quotes eld
\end_inset

quantile
\begin_inset Quotes erd
\end_inset

 approach:
\end_layout

\begin_layout Itemize
It spreads the area covered by the training more evenly.
 This is critical in high dimension case where much of the areas can be
 left uncovered with random sampling.
\end_layout

\begin_layout Itemize
The neural network will potentially perform better in the interval 
\begin_inset Formula $\left[0,1\right]$
\end_inset

 than in the line.
 A good reason to believe that is the tail noise in the normal-marginals
 example.
 To be tested.
\end_layout

\begin_layout Itemize
Potentially easier update of the sampling measure 
\begin_inset Formula $\hat{\theta}$
\end_inset

.
\end_layout

\begin_layout Subparagraph*
Penalization integral approximation - Comparison
\end_layout

\begin_layout Standard
We approximate the penalty term 
\begin_inset Formula $\varepsilon=\int b_{\gamma}\left(c-\varphi\right)d\theta$
\end_inset

 with a sum, as described below.
\end_layout

\begin_layout Enumerate
Sampling.
 Let 
\begin_inset Formula $\mathcal{X}_{i}=\left\{ x_{i}^{1},\ldots x_{i}^{n}\right\} ,\mathcal{Y}_{i}=\left\{ y_{i}^{1},\ldots y_{i}^{n}\right\} $
\end_inset

 be (simulated) samples, 
\begin_inset Formula $i=1,\ldots,d$
\end_inset

.
 Note 1: the marginal probabilities are expressed in the samples; their
 construction is the only place where the marginal probabilities are ever
 considered.
 Note 2: In the empirical example, even though we are using actual market
 data, the method first builds marginal distributions of individual prices
 and then generates a random sample from those distributions.
 Hence, the final sample is simulated in any case.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Current method.
\end_layout

\begin_deeper
\begin_layout Standard
First step is building a common sample 
\begin_inset Formula $\Theta_{1}$
\end_inset

 of 
\begin_inset Formula $2d$
\end_inset

-dimension points.
 In the independent case, we wrap the points arbitrarily as (for instance)
 
\begin_inset Formula $\Theta_{1}=\left\{ \left(x_{1}^{1},\ldots,x_{d}^{1},y_{1}^{1},\ldots,y_{d}^{1}\right),\ldots,\left(x_{1}^{n},\ldots,x_{d}^{n},y_{1}^{n},\ldots,y_{d}^{n}\right)\right\} $
\end_inset

.
 In the monotone coupling case, the 
\begin_inset Formula $x_{i}^{k}$
\end_inset

's are first ordered as 
\begin_inset Formula $x_{i}^{\left(1\right)},\ldots,x_{i}^{\left(n\right)}$
\end_inset

 and then coupled in order, while the 
\begin_inset Formula $y_{i}^{k}$
\end_inset

's are coupled arbitrarily, like 
\begin_inset Formula $\Theta_{1}=\left\{ \left(x_{1}^{\left(1\right)},\ldots,x_{d}^{\left(1\right)},y_{1}^{1},\ldots,y_{d}^{1}\right),\ldots,\left(x_{1}^{\left(n\right)},\ldots,x_{d}^{\left(n\right)},y_{1}^{n},\ldots,y_{d}^{n}\right)\right\} $
\end_inset

.
 The penalty is given by
\begin_inset Formula 
\[
\varepsilon_{1}=\sum_{\left(x,y\right)\in\Theta_{1}}b_{\gamma}\left(c\left(x,y\right)-\varphi\left(x,y\right)\right)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Separated sampling.
 The common sample 
\begin_inset Formula $\Theta_{2}$
\end_inset

 is the product of the marginal samples.
 In the independent coupling case, the penalty is calculated as
\begin_inset Formula 
\[
\varepsilon_{2}=\sum_{x_{1}\in\mathcal{X}_{1}}\ldots\sum_{x_{d}\in\mathcal{X}_{d}}\sum_{y_{1}\in\mathcal{Y}_{1}}\ldots\sum_{y_{d}\in\mathcal{Y}_{d}}b_{\gamma}\left(c\left(x,y\right)-\varphi\left(x,y\right)\right)
\]

\end_inset

In the monotone case, the choice of 
\begin_inset Formula $x$
\end_inset

 can be seen as a function of 
\begin_inset Formula $x_{1}$
\end_inset

, therefore we can write 
\begin_inset Formula 
\[
\varepsilon_{2}=\sum_{x_{1}\in\mathcal{X}_{1}}\sum_{y_{1}\in\mathcal{Y}_{1}}\ldots\sum_{y_{d}\in\mathcal{Y}_{d}}b_{\gamma}\left(c\left(x\left(x_{1}\right),y\right)-\varphi\left(x\left(x_{1}\right),y\right)\right)
\]

\end_inset

To keep the complexity comparable, we must have either 
\begin_inset Formula $\prod_{i}\prod_{j}\left|\mathcal{X}_{i}\right|\left|Y_{i}\right|=n^{2d}$
\end_inset

 (in the independent case) or 
\begin_inset Formula $\left|\mathcal{X}_{1}\right|\prod_{j}\left|Y_{i}\right|=n^{d+1}$
\end_inset

 (in the monotone coupling case) in the same scale as the full sample 
\begin_inset Formula $\Theta_{1}$
\end_inset

 of the previous method.
 Thus, suppose we used 
\begin_inset Formula $n=1M$
\end_inset

 in the first sampling method.
 In the independent case, we should use a comparable 
\begin_inset Formula $n=\left(1M\right)^{\nicefrac{1}{4}}\sim32$
\end_inset

 for 
\begin_inset Formula $d=2$
\end_inset

 and 
\begin_inset Formula $n=\left(1M\right)^{\nicefrac{1}{6}}=10$
\end_inset

 for 
\begin_inset Formula $d=3$
\end_inset

.
 In the monotone coupling case, we should use 
\begin_inset Formula $n=\left(1M\right)^{\nicefrac{1}{3}}=100$
\end_inset

 for 
\begin_inset Formula $d=2$
\end_inset

 and 
\begin_inset Formula $n=\left(1M\right)^{\nicefrac{1}{4}}\sim32$
\end_inset

 for 
\begin_inset Formula $d=3$
\end_inset

.
\end_layout

\begin_layout Standard
Notice that, provided the sample sets have the same size, there is no difference
 between the two methods just discussed in terms of computational effort
 (other that memory allocation, perhaps).
 But in the separated sampling case, given that marginal sizes are small
 specially in higher dimension, we may have some areas of the marginal distribut
ions being scarcelly covered throughout 
\begin_inset Formula $\Theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Quantile method.
 The points 
\begin_inset Formula $\left(\hat{x},\hat{y}\right)$
\end_inset

 are taken from the grid of the 1-hypercube.
 So, for example, if the grid granularity is 10, we divide each marginal
 of the cube into 10 intervals, and the points are the middle points 
\begin_inset Formula 
\begin{align*}
\hat{x}_{i}\in\hat{\mathcal{X}}_{i} & =\left\{ 0.05,\ldots,0.95\right\} \\
\hat{y}_{i}\in\hat{\mathcal{Y}}_{i} & =\left\{ 0.05,\ldots,0.95\right\} 
\end{align*}

\end_inset

and the penalty is
\begin_inset Formula 
\[
\varepsilon_{3}=\sum_{\hat{x}_{1}\in\hat{\mathcal{X}}_{1}}\ldots\sum_{\hat{x}_{d}\in\hat{\mathcal{X}}_{d}}\sum_{\hat{y}_{1}\in\hat{\mathcal{Y}}_{1}}\ldots\sum_{\hat{y}_{d}\in\hat{\mathcal{Y}}_{d}}\hat{\theta}\left(\hat{x},\hat{y}\right)b_{\gamma}\left(c\left(x,y\right)-\varphi\left(x,y\right)\right)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Notice that the sampling measure probability has entered the formula, while
 in the previous method it only influenced the construction of the sample.
 So, while the discrete marginal pro
\begin_inset Formula $F_{i},G_{i}$
\end_inset

 are known, the joint cummulative 
\begin_inset Formula $\mathbf{F}\left(x,y\right)$
\end_inset

 used to calculate the discrete 
\begin_inset Formula $\mathbb{P}\left(\hat{x},\hat{y}\right)$
\end_inset

 used in the penalization method (see details below) is chosen through 
\begin_inset Formula $\hat{\theta}$
\end_inset

.
 In the independent coupling case, 
\begin_inset Formula $\mathbf{F}\left(x,y\right)=\prod F_{i}\left(x_{i}\right)G_{i}\left(y_{i}\right)$
\end_inset

; other cases must be studied.
\end_layout

\end_deeper
\end_body
\end_document
